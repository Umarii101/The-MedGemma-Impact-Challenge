# MedGemma Edge - Android Application

> Android demo app for on-device clinical AI inference

## ðŸš§ Status: In Development

This folder will contain the Android application that demonstrates:

1. **BiomedCLIP inference** â€” Medical image embedding via ONNX Runtime Mobile
2. **MedGemma inference** â€” Clinical text generation via llama.cpp

## Planned Architecture

```
android_app/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ src/main/
â”‚   â”‚   â”œâ”€â”€ java/com/medgemma/edge/
â”‚   â”‚   â”‚   â”œâ”€â”€ MainActivity.kt
â”‚   â”‚   â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ BiomedClipInference.kt
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ MedGemmaInference.kt
â”‚   â”‚   â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ CameraFragment.kt
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ResultsFragment.kt
â”‚   â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â”‚   â”œâ”€â”€ biomedclip_vision_int8.onnx
â”‚   â”‚   â”‚   â””â”€â”€ medgemma-4b-q4_k_s.gguf
â”‚   â”‚   â””â”€â”€ res/
â”‚   â””â”€â”€ build.gradle
â”œâ”€â”€ build.gradle
â””â”€â”€ settings.gradle
```

## Dependencies

```gradle
// ONNX Runtime for BiomedCLIP
implementation 'com.microsoft.onnxruntime:onnxruntime-android:1.16.0'

// llama.cpp for MedGemma (via JNI bindings)
implementation 'com.github.aspect-build:llama-cpp-android:...'
```

## Target Device

**Realme GT Neo 6** (Snapdragon 8s Gen 3)
- 8-12 GB RAM
- Hexagon NPU
- Android 14+

## Model Files

Models are not included in the repository (too large for Git).

Download from:
- `biomedclip_vision_int8.onnx` â€” 84 MB
- `medgemma-4b-q4_k_s.gguf` â€” 2.2 GB

Place in `app/src/main/assets/` before building.

## Build Instructions

```bash
# Open in Android Studio
# Sync Gradle
# Build > Make Project
# Run on device or emulator
```

## Minimum Requirements

- Android SDK 24+ (Android 7.0)
- 8 GB device RAM
- 3 GB storage for models
- Camera permission (for X-ray capture)
